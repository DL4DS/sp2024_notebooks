{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient Calculations and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/DL4DS/sp2024_notebooks/blob/main/lecture/7_Backprop_with_Micrograd_lite_pt1.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "A simplified implementation of a PyTorch like _backpropagation_ and automated differentiation algorithm,\n",
    "e.g. _autograd_,  via a _computation graph_.\n",
    "\n",
    "---\n",
    "The code is based on Andrej Karpathy's [micrograd](https://github.com/karpathy/micrograd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mp\n",
    "from IPython.display import Image, HTML\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building the `Value` Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the TensorFlow `Tensor` object, we build a \n",
    "* data wrapper as a `class` called `Value` and\n",
    "* build in on all the functionality we need to \n",
    "    * run a forward pass and then \n",
    "    * calculate the gradients in each node via a backward pass.\n",
    "\n",
    "From there we could build a Multi-Layer Neural Network (a.k.a. Multi-Layer Perceptron) and train it\n",
    "which will be shared as Part 2 of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Recording Child Nodes and Operations\n",
    "\n",
    "In order to calculate the gradients, we will need to capture the computation graphs. To do that, we'll need to store pointers to the operands of each operation.\n",
    "\n",
    "For visualization purposes we also record the symbol of the operator used, and a label for the node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python doesn't know how to do arithmetic with this new class, so we override methods to tell \n",
    "python how to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value version 1\n",
    "class Value:\n",
    "                                    #     vvvvvvv  vvvvvvvv\n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op # store the operation that created this node\n",
    "        self.label = label # label for the node\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Return a string representation of the object for display\"\"\"\n",
    "        return f\"Value(data={self.data})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)   \n",
    "        out = Value(self.data + other.data, (self, other), '+') # store tuple of children\n",
    "        return out                                      #  ^^^\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*') # store tuple of children\n",
    "        return out                                      #  ^^^\n",
    "    \n",
    "    def relu(self):                                 #  vvvvvv\n",
    "        out = Value(np.maximum(0, self.data), (self,), 'ReLU')\n",
    "        # out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Value(4.0, label='w')\n",
    "x = Value(-3.0, label='x')\n",
    "b = Value(8.0, label='b')\n",
    "\n",
    "wx = w*x ; wx.label = 'wx'\n",
    "f = wx + b ; f.label = 'f'\n",
    "h = f.relu() ; h.label = 'h'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.data, f._prev, f._op, f.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.data, h._prev, h._op, h.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8ab3WOOP391",
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### The Compute Graph\n",
    "\n",
    "We now have enough information stored about the compute graph to visualize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are two functions to:\n",
    "* walk the graph and build sets of all nodes and edges (`trace`) and then \n",
    "* draw them as a directed graph (`draw_dot`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# draw_dot version 1\n",
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "  # builds a set of all nodes and set of all edges in a graph\n",
    "  nodes, edges = set(), set()\n",
    "  def build(v):\n",
    "    if v not in nodes:\n",
    "      nodes.add(v)\n",
    "      for child in v._prev:\n",
    "        edges.add((child, v))\n",
    "        build(child)\n",
    "  build(root)\n",
    "  return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
    "\n",
    "  nodes, edges = trace(root)\n",
    "  for n in nodes:\n",
    "    uid = str(id(n))\n",
    "    # for any value in the graph, create a rectangular ('record') node for it\n",
    "    dot.node(name = uid, label = \"{ %s | data %.4f }\" % (n.label, n.data), shape='record')\n",
    "    if n._op:\n",
    "      # if this value is a result of some operation, create an op node for it\n",
    "      dot.node(name = uid + n._op, label = n._op)\n",
    "      # and connect this no de to it\n",
    "      dot.edge(uid + n._op, uid)\n",
    "\n",
    "  for n1, n2 in edges:\n",
    "    # connect n1 to the op node of n2\n",
    "    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "  return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j3s1BCjMP391",
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "draw_dot(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oaMxcCKSP391",
    "tags": []
   },
   "source": [
    "Note that every value object becomes a node in the graph. The operators are also represented as a kind of fake node so they can be visualized too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lGXCjYq-P391",
    "tags": []
   },
   "outputs": [],
   "source": [
    "nodes, edges = trace(h)\n",
    "print(\"Nodes: \", nodes)\n",
    "print(\"Edges: \", edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1cWIyciP392",
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Recap\n",
    "\n",
    "So far we've built a Value class and associated data structures to capture a computational graph and calculate the output based on the inputs and operations. We'll call this the __forward pass__.\n",
    "\n",
    "But now, we're interested in calculating the gradients with respect to some of the parameters with respect to $h$. \n",
    "\n",
    "So next we'll update our Value class to capture the partial derivative at each node relative to $h$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80fEbziTP392",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Calculating Gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZDgQ7NZP392",
    "tags": []
   },
   "source": [
    "Add a gradient member variable, `grad`, to our class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value version 2\n",
    "class Value:\n",
    "\n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0 # default to 0  <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op # store the operation that created this node\n",
    "        self.label = label # label for the node\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Return a string representation of the object for display\"\"\"\n",
    "        return f\"Value(data={self.data})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)   \n",
    "        out = Value(self.data + other.data, (self, other), '+') # store tuple of children\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)   \n",
    "        out = Value(self.data * other.data, (self, other), '*') # store tuple of children\n",
    "        return out\n",
    "    \n",
    "    def relu(self):         \n",
    "        out = Value(np.maximum(0, self.data), (self,), 'ReLU')\n",
    "        # out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBZR6rldP392",
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "And update `draw_dot()` to show `grad` in the node info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ahr9SeMZP392",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# draw_dot version 2\n",
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "  # builds a set of all nodes and set of all edges in a graph\n",
    "  nodes, edges = set(), set()\n",
    "  def build(v):\n",
    "    if v not in nodes:\n",
    "      nodes.add(v)\n",
    "      for child in v._prev:\n",
    "        edges.add((child, v))\n",
    "        build(child)\n",
    "  build(root)\n",
    "  return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
    "\n",
    "  nodes, edges = trace(root)\n",
    "  for n in nodes:\n",
    "    uid = str(id(n))\n",
    "    # for any value in the graph, create a rectangular ('record') node for it\n",
    "    dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record')\n",
    "    if n._op:\n",
    "      # if this value is a result of some operation, create an op node for it\n",
    "      dot.node(name = uid + n._op, label = n._op)\n",
    "      # and connect this node to it\n",
    "      dot.edge(uid + n._op, uid)\n",
    "\n",
    "  for n1, n2 in edges:\n",
    "    # connect n1 to the op node of n2\n",
    "    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "  return dot        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And reinitialize and redraw..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Value(4.0, label='w')\n",
    "x = Value(-3.0, label='x')\n",
    "b = Value(14.0, label='b')\n",
    "\n",
    "wx = w*x ; wx.label = 'wx'\n",
    "f = wx + b ; f.label = 'f'\n",
    "h = f.relu() ; h.label = 'h'\n",
    "\n",
    "draw_dot(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhEm4yKIP393",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Manual Gradient Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GY5YV3K6P393",
    "tags": []
   },
   "source": [
    "Before we start implementing backpropagation, it is helpful to manually calculate some gradients to better understand the procedure.\n",
    "\n",
    "For the node $h$, we trivially calculate\n",
    "\n",
    "$$\\frac{dh}{dh} = 1.$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GsGq4xDzP393",
    "tags": []
   },
   "outputs": [],
   "source": [
    "h.grad = 1.0\n",
    "draw_dot(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W71k54M8P393",
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "If we go backwards a step in the graph, we see that $h=\\mathrm{ReLU}(f)$. Technically, \n",
    "ReLU is not differentiable at 0, but by convention we calculate\n",
    "\n",
    "$$\\frac{\\partial{h}}{\\partial{f}} = \n",
    "    \\begin{cases}\n",
    "    1 & \\text{if} \\hspace{12pt} f \\gt 0, \\\\\n",
    "    0 & \\text{if} \\hspace{12pt} f \\le 0\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "The figure below illustrates ReLU and its derivative.\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"./Train2ReLUDeriv.svg\" width=\"30%\">\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.grad = (h.data > 0) * h.grad  # = 0 when f.data <= 0; = h.grad when h.data > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And we can redraw the graph above again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Siw6YrqsP393",
    "tags": []
   },
   "outputs": [],
   "source": [
    "draw_dot(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to calculate\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h}{\\partial wx} = \\frac{\\partial h}{\\partial f} \\cdot \\frac{\\partial f}{\\partial wx} \\hspace{1cm} \\text{and} \\hspace{1cm} \\frac{\\partial h}{\\partial b} = \\frac{\\partial h}{\\partial f} \\cdot \\frac{\\partial f}{\\partial b}\n",
    "$$\n",
    "\n",
    "But $f = wx +b$, so\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial wx} = 1 \\hspace{1cm} \\text{and} \\hspace{1cm}  \\frac{\\partial f}{\\partial b} = 1\n",
    "$$\n",
    "\n",
    "In terms of backpropagation you can think of addition node as **simply copying the parent gradients**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wx.grad = 1 * f.grad\n",
    "b.grad = 1 * f.grad\n",
    "draw_dot(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then finally we want to calculate\n",
    "$$\n",
    "\\frac{\\partial h}{\\partial w} = \\frac{\\partial h}{\\partial f} \\cdot \\frac{\\partial f}{\\partial wx} \\cdot \\frac{\\partial wx}{\\partial w} \\hspace{1cm} \\text{and} \\hspace{1cm} \n",
    "\\frac{\\partial h}{\\partial w} = \\frac{\\partial h}{\\partial f} \\cdot \\frac{\\partial f}{\\partial wx} \\cdot \\frac{\\partial wx}{\\partial w}\n",
    "$$\n",
    "\n",
    "But $wx = w * x$ so \n",
    "\n",
    "$$\n",
    "\\frac{\\partial wx}{\\partial w} = x \\hspace{1cm} \\text{and} \\hspace{1cm} \\frac{\\partial wx}{\\partial x} = w\n",
    "$$\n",
    "\n",
    "So in terms of backpropagation, the **gradient of one operand of a product is just the data value of the other operand**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.grad = wx.grad * x.data\n",
    "x.grad = wx.grad * w.data\n",
    "draw_dot(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcq9FkAGP395",
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Recap\n",
    "\n",
    "As you saw, we recursively went backwards through the computation graph and applied the local gradients to the gradients calculated so far to get the partial gradients. Put another we propagated this calculations backwards through the graph.\n",
    "\n",
    "Of course, in practice, we will only need the gradients on the parameters, not the inputs, so we won't bother calculating them on inputs.\n",
    "\n",
    "_That is the essence of Back Propagation._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WZ1VSdKP395",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## A Step in Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the graph again. Assume we want the value of $h$ to _decrease_. \n",
    "\n",
    "We are free to change the values of the _leaf nodes_ -- all the other nodes are derived from children and leaf nodes.\n",
    "\n",
    "The leaf nodes are $w, x$ and $b$. And of those, $w$ and $b$ are our parameter leaf nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KIPB3tFwP395",
    "tags": []
   },
   "outputs": [],
   "source": [
    "draw_dot(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the current value of L."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDOS_8h4P395",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remind ourselves what L is\n",
    "print(h.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we showed before, we want to nudge each of those leaf nodes by the negative\n",
    "of the gradient, multiplied by a step size, $\\alpha$.\n",
    "\n",
    "$$ \\phi_{n+1} = \\phi_n - \\alpha * \\frac{\\partial{L}}{\\partial{\\phi_n}} = \\phi_n - \\alpha * \\sum_{i=1}^I\\frac{\\partial{\\ell_i}}{\\partial{\\phi_n}}$$\n",
    "\n",
    "where $n$ is the iteration number.\n",
    "\n",
    "Also remember, we have to calculate the gradients with respect to _every parameter_ for _every sample_ from our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YToAWL-5P395",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# nudge all the leaf nodes along the negative direction of the gradient\n",
    "step_size = 0.01    # also called eta above\n",
    "\n",
    "w.data -= step_size * w.grad\n",
    "b.data -= step_size * b.grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run a forward pass with our updated parameters and see what happened to $h$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wx = w*x\n",
    "f = wx + b\n",
    "h = f.relu()\n",
    "\n",
    "print(h.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1cBugc3P396",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## A Single Neuron with Two Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now  define a single neuron with\n",
    "* two inputs\n",
    "* two weights (1 for each input)\n",
    "* a bias\n",
    "* the ReLU activation function\n",
    "\n",
    "Recall the neuron figure above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jzqdgbSbP396",
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assign input values\n",
    "\n",
    "# inputs x0, x1\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the weights and bias\n",
    "\n",
    "# weights w1, w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "\n",
    "# bias of the neuron\n",
    "b = Value(6.8813735870195432, label='b')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the forward pass\n",
    "\n",
    "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1w1 + x2w2'\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "o = n.relu(); o.label = 'o'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-2AFiyyrP396",
    "tags": []
   },
   "outputs": [],
   "source": [
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjQqsriiP396",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Coding Backpropagation\n",
    "\n",
    "Now we'll update our `Value` class once more to support the backward pass.\n",
    "\n",
    "There's a\n",
    "* private `_backward()` function _in each operator_ that implements the local\n",
    "step of the chain rule, and\n",
    "* a `backward()` function in the class that topologically sorts the graph and calls the operator `_backward()` function starting at the end of the graph and going _backward_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "51yG2gFAP396",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# version 3\n",
    "class Value:\n",
    "\n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0 # default to 0, no impact on the output\n",
    "        self._backward = lambda: None  # by default backward doesn't do anything\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Return a string representation of the object for display\"\"\"\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def relu(self):\n",
    "        out = Value(np.maximum(0, self.data), (self,), 'ReLU')\n",
    "        # out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (out.data > 0) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        # topological order all of the children in the graph\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        # go one variable at a time and apply the chain rule to get its gradient\n",
    "        self.grad = 1\n",
    "        for v in reversed(topo):\n",
    "            v._backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We redefined the class so we have to reinitialize the objects and run the operations again.\n",
    "\n",
    "This constitutes the _forward pass_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lb5q0OtZP397",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the input values\n",
    "\n",
    "# inputs x0, x1\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the weights and bias\n",
    "\n",
    "# weights w1, w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "\n",
    "# bias of the neuron\n",
    "#b = Value(6.7, label='b')\n",
    "b = Value(6.8813735870195432, label='b')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run the forward pass\n",
    "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1w1 + x2w2'\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "o = n.relu(); o.label = 'o'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've filled the data values for all the nodes, but haven't calculated the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0xjnpuVP397",
    "tags": []
   },
   "outputs": [],
   "source": [
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, all we have to do is call the `backward()` method of the last node..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And voila! We have all the gradients!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nudge all the leaf nodes along the negative direction of the gradient\n",
    "step_size = 0.01    # also called eta above\n",
    "\n",
    "w1.data -= step_size * w1.grad\n",
    "w2.data -= step_size * w2.grad\n",
    "b.data -= step_size * b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run the forward pass\n",
    "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1w1 + x2w2'\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "o = n.relu(); o.label = 'o'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see the parameters have been updated and the output has been reduced a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.backward()\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eS7x_XhbP398",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Accumulating the Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observant viewer will notice that we are accumulating the gradients in the `_backward()` functions.\n",
    "\n",
    "As we showed before, we want to nudge each of those leaf nodes by the negative\n",
    "of the _sum of the gradients for every sample in our training set_.\n",
    "\n",
    "$$ \\phi_{n+1} = \\phi_n - \\alpha * \\frac{\\partial{L}}{\\partial{\\phi_n}} = \\phi_n - \\alpha * \\sum_{i=1}^I\\frac{\\partial{\\ell_i}}{\\partial{\\phi_n}}$$\n",
    "\n",
    "where $i$ is the sample index and $n$ is the iteration number.\n",
    "\n",
    "So remember we are backpropagating on the sum of the losses\n",
    "$$ \n",
    "\\frac{\\partial{L}}{\\partial{\\phi_n}} = \\sum_{i=1}^I\\frac{\\partial{\\ell_i}}{\\partial{\\phi_n}}\n",
    "$$\n",
    "\n",
    "So after we finish calculating gradients on the batch, we have to make sure to \n",
    "zero the gradients before we start the next batch.\n",
    "\n",
    "Below we do our final update to the `Value` class to add a `zero_grad()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 4\n",
    "class Value:\n",
    "\n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0 # default to 0, no impact on the output\n",
    "        self._backward = lambda: None  # by default backward doesn't do anything\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Return a string representation of the object for display\"\"\"\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def relu(self):\n",
    "        out = Value(np.maximum(0, self.data), (self,), 'ReLU')\n",
    "        # out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (out.data > 0) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        # topological order all of the children in the graph\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        # go one variable at a time and apply the chain rule to get its gradient\n",
    "        self.grad = 1\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n",
    "    # New method to zero out the gradients\n",
    "    def zero_grad(self):\n",
    "\n",
    "        # topological order all of the children in the graph\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        # zero out the gradient for each node\n",
    "        self.grad = 0\n",
    "        for v in topo:\n",
    "            v.grad = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the weights and bias\n",
    "\n",
    "# weights w1, w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "\n",
    "# bias of the neuron\n",
    "#b = Value(6.7, label='b')\n",
    "b = Value(6.8813735870195432, label='b')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the input values\n",
    "\n",
    "# inputs x0, x1\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run the forward pass\n",
    "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1w1 + x2w2'\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "o = n.relu(); o.label = 'o'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set a second set of input values.\n",
    "\n",
    "# inputs x0, x1\n",
    "x1 = Value(1.0, label='x1')\n",
    "x2 = Value(-1.0, label='x2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run the forward pass again\n",
    "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1w1 + x2w2'\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "o = n.relu(); o.label = 'o'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nudge all the leaf nodes along the negative direction of the gradient\n",
    "step_size = 0.01    # also called eta above\n",
    "\n",
    "w1.data -= step_size * w1.grad\n",
    "w2.data -= step_size * w2.grad\n",
    "b.data -= step_size * b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run the forward pass again\n",
    "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1w1 + x2w2'\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "o = n.relu(); o.label = 'o'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So before the next batch, we have to zero our gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.zero_grad()\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Case Needs for Gradient Accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We also need to accumulate gradients to handle cases like where a `Value` object is on both sides of the operand like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10ZTwAWSP398",
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = Value(3.0, label='a')\n",
    "b = a + a ; b.label = 'b'\n",
    "b.backward()\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we didn't have the accumulation, then `a.grad = 1` instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CBZ9-vsP398",
    "tags": []
   },
   "source": [
    "Or the other case where a node goes to different operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QvRJ9LFZP399",
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = Value(-2.0, label='a')\n",
    "b = Value(3.0, label='b')\n",
    "d = a * b  ; d.label = 'd'\n",
    "e = a + b   ; e.label = 'e'\n",
    "  \n",
    "draw_dot(e) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see analytical verification of the above result in the note in the online book.\n",
    "\n",
    "The risk now is that if you don't zero the gradients for the next update iteration, you will have incorrect gradients. \n",
    "\n",
    "> Always remember to zero the gradient in each iteration of the training loop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mU4SFgwAP39_",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Comparing to PyTorch\n",
    "\n",
    "We're using a class implementation that resembles the PyTorch implementation, and in fact we can compare our implementation with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSFutF8zP39_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "51dJ5c74P39_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "x1 = torch.Tensor([2.0]).double()                ; x1.requires_grad = True\n",
    "x2 = torch.Tensor([0.0]).double()                ; x2.requires_grad = True\n",
    "w1 = torch.Tensor([-3.0]).double()               ; w1.requires_grad = True\n",
    "w2 = torch.Tensor([1.0]).double()                ; w2.requires_grad = True\n",
    "b = torch.Tensor([6.8813735870195432]).double()  ; b.requires_grad = True\n",
    "n = x1*w1 + x2*w2 + b\n",
    "o = torch.relu(n)\n",
    "\n",
    "print(o.data.item())\n",
    "o.backward()\n",
    "\n",
    "print('---')\n",
    "print('x2.grad', x2.grad.item())\n",
    "print('w2.grad', w2.grad.item())\n",
    "print('x1.grad', x1.grad.item())\n",
    "print('w1.grad', w1.grad.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlcUXqEmP39_",
    "tags": []
   },
   "source": [
    "By default, tensors don't store gradients and so won't support backprop, so we explicitly set `requires_grad = True`."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
