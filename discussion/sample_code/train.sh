#!/bin/bash -l

# Set SCC project
#$ -P ds598

# Assuming you want to run a job array with 8 combinations (2 batch sizes x 2 learning rates x 2 epochs, you would need to set this number manually)
#$ -t 1-8

module load miniconda/4.9.2
conda activate ds598 # activate your conda environment

# Define hyperparameter arrays (Define all your hyperparameters here)
batch_sizes=(32 64)
learning_rates=(0.001 0.0001)
epochs=(5 10)

# The Code below will then create an array of combinations
declare -a combinations
index=0
for bs in "${batch_sizes[@]}"; do
    for lr in "${learning_rates[@]}"; do
        for epoch in "${epochs[@]}"; do
            combinations[index]="$bs,$lr,$epoch"
            ((index++))
        done
    done
done

# Extract the combination for the current SGE_TASK_ID (SGE_TASK_ID is an ID that is automatically generated by the job scheduler, it is used to index the combinations array)
# The combination is then split into the individual hyperparameters and passed to the main.py script
IFS=',' read -r bs lr epoch <<< "${combinations[$SGE_TASK_ID-1]}"

echo "Running training with bs=$bs, lr=$lr, epochs=$epoch"
python main.py --bs "$bs" --lr "$lr" --epochs "$epoch" --model_name resnet18

# Notes for job submission:
# To submit the job to SCC, run the following command in the terminal:
# qsub -pe omp 4 -P ds598 -l gpus=1 train.sh

# After submission, you can check the status of your job with:
# qstat -u $USER
# This will show you whether your jobs are waiting in the queue (state qw) or running (state r).
