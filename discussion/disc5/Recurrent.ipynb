{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap (More in the Lectures)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recurrent Neural Networks\n",
    "\n",
    "- Why the need for RNNs?\n",
    "    - Feedforward neural networks are not designed to handle sequential/temporal data.\n",
    "    - Feedforward neural networks (and Convolutional neural networks), can only take in a fixed sized vector as input and produce a fixed sized vector as output, using a fixed size of computational steps (layers). Recurrent Nets allows us to operate over sequences of vectors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](http://karpathy.github.io/assets/rnn/diags.jpeg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Traditional Feed Forward Neural Network    \n",
    "(2) Sequence Output (Image Captioning)    \n",
    "(3) Sequence Input (Sentiment Analysis)    \n",
    "(4) Sequence Input and Sequence Output (Machine Translation)    \n",
    "(5) Synced Sequence Input and Output (Video Classification)    \n",
    "\n",
    "Note: RNNs utilize the same set of weights at each time step, thus the number of parameters do not increase with the size of the input. This is a significant advantage when dealing with long  and variable length sequences."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an RNN for a Character-Level Language Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What goes into the model:\n",
    "- X: A sequence of characters (eg. a sentence, a document, a word)\n",
    "- Example: \"Hi, Hell\"\n",
    "\n",
    "#### What will the model do:\n",
    "- Learn to predict the next character in the sequence. That is, we get the model to give the probability distribution of the next character in the sequence given the sequence of previous characters.\n",
    "- Example: Given the sequence \"Hi, Hell\", the model should predict the next character as \"o\" with high probability.\n",
    "- The model continues to predict the next character given the sequence of previous characters and the predicted characters.\n",
    "\n",
    "Another Example:\n",
    "\n",
    "Suppose we only had a vocabulary of four possible letters “helo”, and wanted to train an RNN on the training sequence “hello”. This training sequence is in fact a source of 4 separate training examples: \n",
    "1. The probability of “e” should be likely given the context of “h”, \n",
    "2. “l” should be likely in the context of “he”, \n",
    "3. “l” should also be likely given the context of “hel”, and finally \n",
    "4. “o” should be likely given the context of “hell”.\n",
    "\n",
    "Image bellow: RNN is passed the sequence \"hell\" and it should predict the next character as \"o\" with high probability. (Which it does, highest score is at index 3 for \"o\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](http://karpathy.github.io/assets/rnn/charseq.jpeg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is training done?\n",
    "\n",
    "- We train the model to minimize the cross-entropy loss between the true distribution of the next character and the predicted distribution.\n",
    "- Example: When the model gets the first character 'h' (at the first time step), it should predict the next character 'e' with high probability. Thereby, we tweak the model's parameters such that it gets better at predicting the next character ('e') given the first character ('h'). And so on for the rest of the sequence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "- Notice also that the first time the character “l” is input, the target is “l”, but the second time the target is “o”. The RNN therefore cannot rely on the input alone and must use its recurrent connection to keep track of the context to achieve this task."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is testing/eval done?\n",
    "\n",
    "- At test time, we feed a character into the RNN and get a distribution over what characters are likely to come next. We sample from this distribution, and feed it right back in to get the next letter. Repeat this process and you’re sampling text!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with RNNs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://kharshit.github.io/img/gradient_flow_rnn.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs have the following structure:\n",
    "\n",
    "$$h_t = f_W(h_{t-1}, x_t)$$\n",
    "\n",
    "Here, the same function and same set of parameters are used at every time step. $h_t$ denotes the state (which is kind of like the memory/summary of past sequence of inputs) of the RNN at time t. \n",
    "\n",
    "The gradient flows backwards through all the time steps (from the loss at the end to the first time step). This is called backpropagation through time (BPTT). This means the gradient computation involves recurrent multiplication of $W$. This can lead to the following problems:\n",
    "\n",
    "- Vanishing Gradients: If the recurrent weight matrix has small singular values, the gradients will vanish as we backpropagate to earlier time steps. This means that the model is not able to capture long range dependencies in the data.\n",
    "\n",
    "- Exploding Gradients: If the recurrent weight matrix has large singular values, the gradients will explode as we backpropagate to earlier time steps. This means that the model will not be able to learn effectively. Could be solved by clipping the gradients. (i.e. if gradient is larger than a threshold, clip it to the threshold)\n",
    "\n",
    "\n",
    "### Long Short Term Memory (LSTM) Networks\n",
    "- aims to solve the vanishing gradient problem\n",
    "- More Later"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xn5kA92_J5KLaKcP7BMRLA.gif)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*goJVQs-p9kgLODFNyhl9zA.gif)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0f8r3Vd-i4ueYND1CUrhMA.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommended Read: \n",
    "\n",
    "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\n",
    "\n",
    "https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n",
    "        super(RNNModel, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # RNN layers\n",
    "        self.rnn = nn.RNN(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :] # Get the last time step output\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :] # Get the last time step output\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read document\n",
    "with open ('data/tinyshakespeare.txt', 'r') as f:\n",
    "    doc = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SequentialModule(nn.Module):\n",
    "    # initialize module\n",
    "    def __init__(self, n_vocab, seq_size=32, embedding_size=64, lstm_size=64):\n",
    "        super(SequentialModule, self).__init__()\n",
    "        self.seq_size = seq_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size,\n",
    "                            lstm_size,\n",
    "                            batch_first=True)\n",
    "        self.dense = nn.Linear(lstm_size, n_vocab)\n",
    "        \n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.dense(output)\n",
    "\n",
    "        return logits, state\n",
    "    \n",
    "    def zero_state(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.lstm_size),torch.zeros(1, batch_size, self.lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "seq_size = 32\n",
    "embedding_size = 64\n",
    "lstm_size = 64\n",
    "gradients_norm = 5\n",
    "# set device parameter\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of words from document\n",
    "def doc2words(doc):\n",
    "    lines = doc.split('\\n')\n",
    "    lines = [line.strip(r'\\\"') for line in lines]\n",
    "    words = ' '.join(lines).split()\n",
    "    return words\n",
    "\n",
    "def removepunct(words):\n",
    "    punct = set(string.punctuation)\n",
    "    words = [''.join([char for char in list(word) if char not in punct]) for word in words]\n",
    "    return words\n",
    "\n",
    "# get vocab from word list\n",
    "def getvocab(words):\n",
    "    wordfreq = Counter(words)\n",
    "    sorted_wordfreq = sorted(wordfreq, key=wordfreq.get)\n",
    "    return sorted_wordfreq\n",
    "\n",
    "# get dictionary of int to words and word to int\n",
    "def vocab_map(vocab):\n",
    "    int_to_vocab = {k:w for k,w in enumerate(vocab)}\n",
    "    vocab_to_int = {w:k for k,w in int_to_vocab.items()}\n",
    "    return int_to_vocab, vocab_to_int\n",
    "\n",
    "words = removepunct(doc2words(doc))\n",
    "vocab = getvocab(words)\n",
    "int_to_vocab, vocab_to_int = vocab_map(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(words, vocab_to_int, batch_size, seq_size):\n",
    "    # generate a Xs and Ys of shape (batchsize * num_batches) * seq_size\n",
    "    word_ints = [vocab_to_int[word] for word in words]\n",
    "    num_batches = int(len(word_ints) / (batch_size * seq_size))\n",
    "    Xs = word_ints[:num_batches*batch_size*seq_size]\n",
    "    Ys = np.zeros_like(Xs)\n",
    "    Ys[:-1] = Xs[1:]\n",
    "    Ys[-1] = Xs[0]\n",
    "    Xs = np.reshape(Xs, (num_batches*batch_size, seq_size))\n",
    "    Ys= np.reshape(Ys, (num_batches*batch_size, seq_size))\n",
    "    \n",
    "    # iterate over rows of Xs and Ys to generate batches\n",
    "    for i in range(0, num_batches*batch_size, batch_size):\n",
    "        yield Xs[i:i+batch_size, :], Ys[i:i+batch_size, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_and_train_op(net, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    return criterion, optimizer\n",
    "    \n",
    "def generate_text(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k=5):\n",
    "    net.eval()\n",
    "\n",
    "    state_h, state_c = net.zero_state(1)\n",
    "    state_h = state_h.to(device)\n",
    "    state_c = state_c.to(device)\n",
    "    for w in words:\n",
    "        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
    "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
    "    \n",
    "    _, top_ix = torch.topk(output[0], k=top_k)\n",
    "    choices = top_ix.tolist()\n",
    "    choice = np.random.choice(choices[0])\n",
    "\n",
    "    words.append(int_to_vocab[choice])\n",
    "    \n",
    "    for _ in range(100):\n",
    "        ix = torch.tensor([[choice]]).to(device)\n",
    "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
    "\n",
    "        _, top_ix = torch.topk(output[0], k=top_k)\n",
    "        choices = top_ix.tolist()\n",
    "        choice = np.random.choice(choices[0])\n",
    "        words.append(int_to_vocab[choice])\n",
    "\n",
    "    print(' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(words, vocab_to_int, int_to_vocab, n_vocab):\n",
    "    \n",
    "    # RNN instance\n",
    "    net = SequentialModule(n_vocab, seq_size, embedding_size, lstm_size)\n",
    "    net = net.to(device)\n",
    "    criterion, optimizer = get_loss_and_train_op(net, 0.01)\n",
    "\n",
    "    iteration = 0\n",
    "    \n",
    "    for e in range(10):\n",
    "        batches = get_batches(words, vocab_to_int, batch_size, seq_size)\n",
    "        state_h, state_c = net.zero_state(batch_size)\n",
    "\n",
    "        # Transfer data to GPU\n",
    "        state_h = state_h.to(device)\n",
    "        state_c = state_c.to(device)\n",
    "        for x, y in batches:\n",
    "            iteration += 1\n",
    "\n",
    "            # Tell it we are in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Reset all gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Transfer data to GPU\n",
    "            x = torch.tensor(x).to(device)\n",
    "            y = torch.tensor(y).to(device)\n",
    "\n",
    "            logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
    "            loss = criterion(logits.transpose(1, 2), y)\n",
    "\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "\n",
    "            loss_value = loss.item()\n",
    "\n",
    "            # Perform back-propagation\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            _ = torch.nn.utils.clip_grad_norm_(net.parameters(), gradients_norm)\n",
    "            \n",
    "            # Update the network's parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            if iteration % 100 == 0:\n",
    "                print('Epoch: {}/{}'.format(e, 10),'Iteration: {}'.format(iteration),'Loss: {}'.format(loss_value))\n",
    "\n",
    "            # if iteration % 1000 == 0:\n",
    "                # predict(device, net, flags.initial_words, n_vocab,vocab_to_int, int_to_vocab, top_k=5)\n",
    "                # torch.save(net.state_dict(),'checkpoint_pt/model-{}.pth'.format(iteration))\n",
    "                \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10 Iteration: 100 Loss: 7.231979846954346\n",
      "Epoch: 0/10 Iteration: 200 Loss: 6.964202880859375\n",
      "Epoch: 0/10 Iteration: 300 Loss: 6.74375057220459\n",
      "Epoch: 1/10 Iteration: 400 Loss: 6.750913619995117\n",
      "Epoch: 1/10 Iteration: 500 Loss: 5.9644694328308105\n",
      "Epoch: 1/10 Iteration: 600 Loss: 6.4356184005737305\n",
      "Epoch: 1/10 Iteration: 700 Loss: 6.3372111320495605\n",
      "Epoch: 2/10 Iteration: 800 Loss: 6.194395542144775\n",
      "Epoch: 2/10 Iteration: 900 Loss: 5.7319560050964355\n",
      "Epoch: 2/10 Iteration: 1000 Loss: 5.853903770446777\n",
      "Epoch: 2/10 Iteration: 1100 Loss: 5.552889823913574\n",
      "Epoch: 3/10 Iteration: 1200 Loss: 5.788702011108398\n",
      "Epoch: 3/10 Iteration: 1300 Loss: 5.7780680656433105\n",
      "Epoch: 3/10 Iteration: 1400 Loss: 5.656917095184326\n",
      "Epoch: 3/10 Iteration: 1500 Loss: 5.560693740844727\n",
      "Epoch: 4/10 Iteration: 1600 Loss: 5.155842304229736\n",
      "Epoch: 4/10 Iteration: 1700 Loss: 5.145718097686768\n",
      "Epoch: 4/10 Iteration: 1800 Loss: 5.2600860595703125\n",
      "Epoch: 4/10 Iteration: 1900 Loss: 5.132971286773682\n",
      "Epoch: 5/10 Iteration: 2000 Loss: 5.308545112609863\n",
      "Epoch: 5/10 Iteration: 2100 Loss: 5.34194803237915\n",
      "Epoch: 5/10 Iteration: 2200 Loss: 5.2131571769714355\n",
      "Epoch: 5/10 Iteration: 2300 Loss: 4.572067737579346\n",
      "Epoch: 6/10 Iteration: 2400 Loss: 5.019961833953857\n",
      "Epoch: 6/10 Iteration: 2500 Loss: 4.888901233673096\n",
      "Epoch: 6/10 Iteration: 2600 Loss: 4.823285102844238\n",
      "Epoch: 6/10 Iteration: 2700 Loss: 4.63541316986084\n",
      "Epoch: 7/10 Iteration: 2800 Loss: 4.929285526275635\n",
      "Epoch: 7/10 Iteration: 2900 Loss: 4.619411468505859\n",
      "Epoch: 7/10 Iteration: 3000 Loss: 4.903716564178467\n",
      "Epoch: 7/10 Iteration: 3100 Loss: 4.672457218170166\n",
      "Epoch: 8/10 Iteration: 3200 Loss: 4.460766792297363\n",
      "Epoch: 8/10 Iteration: 3300 Loss: 4.8532915115356445\n",
      "Epoch: 8/10 Iteration: 3400 Loss: 4.691725730895996\n",
      "Epoch: 8/10 Iteration: 3500 Loss: 4.419217109680176\n",
      "Epoch: 9/10 Iteration: 3600 Loss: 4.475979804992676\n",
      "Epoch: 9/10 Iteration: 3700 Loss: 4.758672714233398\n",
      "Epoch: 9/10 Iteration: 3800 Loss: 4.619863033294678\n",
      "Epoch: 9/10 Iteration: 3900 Loss: 4.634983062744141\n"
     ]
    }
   ],
   "source": [
    "rnn_net = train_rnn(words, vocab_to_int, int_to_vocab, len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today marks you sir what you do thee no of my masters use it was the duke is that I have no screen between thee to my heart LUCENTIO Tranio and my father Pedant I pray Bianca And I pray between your hands the devils musician your name and beat him from the house of the street o it on thee and so I have seen a farmers ground was a very apoplexy pound or the rest that eer that you are choleric Take thy life and therefore shall not have taen him Pedant Sir let the time KATHARINA No my son Lucentio\n"
     ]
    }
   ],
   "source": [
    "generate_text(device, rnn_net, ['Today', 'marks'], len(vocab), vocab_to_int, int_to_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search:\n",
    "\n",
    "Beam search is a strategy used in algorithms that need to make a series of choices from potentially very large sets of options, such as generating sentences word by word in natural language processing.\n",
    "\n",
    "At every step, predicting a different word, could lead the text generated in a completely different path, and most likely there are far too many paths for the model to explore all of them. \n",
    "\n",
    "Beam search helps by keeping track of a limited number of promosing paths (incomplete sentences), at each step, and extends those paths with new words (until the end of the sequence or maximum length is reached).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy Search\n",
    "\n",
    "![title](https://huggingface.co/blog/assets/02_how-to-generate/greedy_search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from the word \"The\", the algorithm greedily chooses the next word of highest probability \"nice\", and so on, so that the final generated word sequence is (\"The\", \"nice\", \"woman\") having an overall probability of 0.5 × 0.4 = 0.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://huggingface.co/blog/assets/02_how-to-generate/beam_search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At time step 1, besides the most likely hypothesis (\"The\", \"nice\"), beam search also keeps track of the second most likely one (\"The\", \"dog\"). At time step 2, beam search finds that the word sequence (\"The\", \"dog\", \"has\") has with 0.36 a higher probability than (\"The\", \"nice\", \"woman\"), which has 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Beam Width: 3\n",
      "Generated Text: As the sun set over the horizon, the sky was filled with clouds.\n",
      "\n",
      "\"What's going on here?\"\n",
      "\n",
      "\"I don't know.\"\n",
      "\n",
      "\"What's going on here?\"\n",
      "\n",
      "\"I don't know.\"\n",
      "\n",
      "\"What's going on?\"\n",
      "\n",
      "Beam Width: 5\n",
      "Generated Text: As the sun set over the horizon, the sky was filled with light.\n",
      "\n",
      "\"It's been a long time since I've seen anything like this.\"\n",
      ".\n",
      "\n",
      "Beam Width: 10\n",
      "Generated Text: As the sun set over the horizon, there was a flash of light in the middle of the street.\n",
      "\n",
      "\"What's going on?\" I asked.\n",
      ".\n",
      "\n",
      "Beam Width: 20\n",
      "Generated Text: As the sun set over the horizon, there was nowhere to be found.\n",
      "\n",
      "There was nowhere to hide.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "def generate_text_beam_search(model, tokenizer, prompt_text, beam_width=5, max_length=100):\n",
    "    def beam_search_step(input_ids, cum_log_probs):\n",
    "        outputs = model(input_ids=input_ids)\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        log_probs, next_tokens = torch.topk(probs, beam_width, dim=-1)\n",
    "        return log_probs, next_tokens\n",
    "\n",
    "    input_ids = tokenizer.encode(prompt_text, return_tensors='pt')\n",
    "    beam_candidates = [(input_ids, 0.0, 0)]  # (input_ids, cumulative_log_prob, length)\n",
    "    completed_sequences = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step in range(max_length):\n",
    "            all_candidates = []\n",
    "            for input_ids, cum_log_prob, length in beam_candidates:\n",
    "                log_probs, next_tokens = beam_search_step(input_ids, cum_log_prob)\n",
    "                for i in range(beam_width):\n",
    "                    new_input_ids = torch.cat([input_ids, next_tokens[:, i].unsqueeze(0)], dim=-1)\n",
    "                    new_cum_log_prob = cum_log_prob + log_probs[:, i].item()\n",
    "                    new_length = length + 1\n",
    "\n",
    "                    if next_tokens[:, i].item() == tokenizer.eos_token_id:\n",
    "                        perplexity = torch.exp(torch.tensor(-new_cum_log_prob) / new_length)\n",
    "                        completed_sequences.append((new_input_ids, perplexity))\n",
    "                    else:\n",
    "                        all_candidates.append((new_input_ids, new_cum_log_prob, new_length))\n",
    "            beam_candidates = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "            if len(completed_sequences) >= 1:  # Break if at least one sequence is completed\n",
    "                break\n",
    "    \n",
    "    # beam_candidate_sentences = [tokenizer.decode(candidate[0][0], skip_special_tokens=True) for candidate in beam_candidates]\n",
    "    # Choose the sequence with the lowest perplexity among completed sequences\n",
    "    if completed_sequences:\n",
    "        best_sequence = min(completed_sequences, key=lambda x: x[1])\n",
    "        return tokenizer.decode(best_sequence[0][0], skip_special_tokens=True), best_sequence[1].item()\n",
    "    else:\n",
    "        # If no sequences were completed, return the best of the current candidates\n",
    "        best_sequence = min(beam_candidates, key=lambda x: x[1])\n",
    "        return tokenizer.decode(best_sequence[0][0], skip_special_tokens=True)\n",
    "\n",
    "# Initialization\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', cache_dir='/projectnb/ds598/projects/xthomas/misc')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2', cache_dir='/projectnb/ds598/projects/xthomas/misc')\n",
    "\n",
    "\n",
    "# Generating text with different beam widths\n",
    "prompt = \"As the sun set over the horizon,\"\n",
    "beam_widths = [3, 5, 10, 20]\n",
    "max_length = 50\n",
    "\n",
    "for width in beam_widths:\n",
    "    print(f\"\\nBeam Width: {width}\")\n",
    "    text, perplexity = generate_text_beam_search(model, tokenizer, prompt, beam_width=width, max_length=max_length)\n",
    "    print(f\"Generated Text: {text}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl4ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
