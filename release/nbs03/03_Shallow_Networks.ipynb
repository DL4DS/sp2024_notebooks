{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this notebook in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel $\\rightarrow$ Restart) and then **run all cells** (in the menubar, select Cell $\\rightarrow$ Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Understanding Deep Learning_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "1Z6LB4Ybn1oN",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ab7bdba0035a4a0fdb2efb1f0dbc3fe1",
     "grade": false,
     "grade_id": "cell-809f6c3c271f85f7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Shallow Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "view-in-github",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9142ae77d08ba1382b5be9fe71574952",
     "grade": false,
     "grade_id": "cell-a581c4cfe13ddfe5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DL4DS/sp2024_notebooks/blob/main/release/nbs03/03_Shallow_Networks.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "66c184714e442e3d0c1866ed92f3c9b4",
     "grade": false,
     "grade_id": "cell-fb6e410802ce2a2b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "The purpose of this notebook is to gain some familiarity with shallow neural networks.  It explores using different numbers of inputs and outputs, hidden units and activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "hAM55ZjSncOk",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fae598ee5221b712459ffe070bebb497",
     "grade": false,
     "grade_id": "cell-8043867dfb671621",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Imports math library\n",
    "import numpy as np\n",
    "# Imports plotting library\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ff34404b2cb713ff22ed191cabbb8aa8",
     "grade": false,
     "grade_id": "cell-e314406b1b8c9549",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Build a Shallow Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "wQDy9UzXpnf5",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0915a53cca38b3508ac02ffc4a81841c",
     "grade": false,
     "grade_id": "cell-2b6bf42cff2d101d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's first construct the shallow neural network with one input, three hidden units, and one output described in section 3.1 of the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "OT7h7sSwpkrt",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "511eac246d1588004be73af51da3795f",
     "grade": false,
     "grade_id": "cell-46d50da3e7950bab",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the Rectified Linear Unit (ReLU) function\n",
    "def ReLU(preactivation):\n",
    "  # Write code to implement the ReLU and compute the activation at the \n",
    "  # hidden unit from the preactivation.\n",
    "  # This should work on every element of the ndarray \"preactivation\" at once\n",
    "  # One way to do this is with the ndarray \"clip\" function\n",
    "  # https://numpy.org/doc/stable/reference/generated/numpy.ndarray.clip.html\n",
    "\n",
    "  # YOUR CODE HERE\n",
    "  raise NotImplementedError()\n",
    "\n",
    "  return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3044ece142817b10bd93f137f9a3e696",
     "grade": false,
     "grade_id": "cell-fd5e9724fe9dff45",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's test your ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6c88fdbafb75fb546c2a3fcd634220ba",
     "grade": true,
     "grade_id": "cell-7fce5169a2450aa7",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test for correctness\n",
    "preactivation = np.array([-1, -0.5, 0, 0.5, 1])\n",
    "assert np.array_equal(ReLU(preactivation), np.array([0, 0, 0, 0.5, 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "12cd136999de22a8b05f567022711ec2",
     "grade": false,
     "grade_id": "cell-45fec6d51c827217",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we'll plot it for $ -5 \\le x \\le 5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "okwJmSw9pVNF",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fea9a1d47204fd41b47eb1924a39e469",
     "grade": false,
     "grade_id": "cell-865a03d4adf76832",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Make an array of inputs\n",
    "z = np.arange(-5,5,0.1)\n",
    "RelU_z = ReLU(z)\n",
    "\n",
    "# Plot the ReLU function\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(z,RelU_z,'r-')\n",
    "ax.set_xlim([-5,5]);ax.set_ylim([-5,5])\n",
    "ax.set_xlabel('z'); ax.set_ylabel('ReLU[z]')\n",
    "ax.set_aspect('equal')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "epk68ZCBu7uJ",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8cf8b0faef54d8d91ffb1e05e1dabe7",
     "grade": false,
     "grade_id": "cell-a52cb470a835e1e2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define a shallow neural network with, one input, one output, and three hidden units\n",
    "def shallow_1_1_3(x, activation_fn, \n",
    "                  phi_0,phi_1,phi_2,phi_3, \n",
    "                  theta_10, theta_11, \n",
    "                  theta_20, theta_21, \n",
    "                  theta_30, theta_31):\n",
    "  \n",
    "  # Note x is a scalar value.\n",
    "\n",
    "  # Assign 3 variables, pre_1, pre_2, pre_3 to hold the preactivations, e.g.\n",
    "  # the linear models based on x and theta values as in figure 3.3 a-c.\n",
    "\n",
    "  # YOUR CODE HERE\n",
    "  raise NotImplementedError()\n",
    "\n",
    "  # Pass these through the activation function to compute the activations as in \n",
    "  # figure 3.3 d-f\n",
    "\n",
    "  act_1 = activation_fn(pre_1)\n",
    "  act_2 = activation_fn(pre_2)\n",
    "  act_3 = activation_fn(pre_3)\n",
    "\n",
    "  # Now weight the activations using phi1, phi2 and phi3 to create the\n",
    "  # weighted activations as in figure 3.3 g-i and add the offset to create \n",
    "  # the output as in figure 3.3j.\n",
    "\n",
    "  # YOUR CODE HERE\n",
    "  raise NotImplementedError()\n",
    "\n",
    "  # Return everything we have calculated\n",
    "  return y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68b7aa335b8effcdbcb5fb8ce9864255",
     "grade": true,
     "grade_id": "cell-eb39d7876a4c7677",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Assign parameter values and test the neural network\n",
    "theta_10 =  0.3 ; theta_11 = -1.0\n",
    "theta_20 = -1.0  ; theta_21 = 2.0\n",
    "theta_30 = -0.5  ; theta_31 = 0.65\n",
    "phi_0 = -0.3; phi_1 = 2.0; phi_2 = -1.0; phi_3 = 7.0\n",
    "\n",
    "# Define a range of input values\n",
    "x = np.array([0.77])\n",
    "\n",
    "# We run the neural network for each of these input values\n",
    "y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3 = \\\n",
    "    shallow_1_1_3(x, ReLU, phi_0,phi_1,phi_2,phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31)\n",
    "\n",
    "# Assuming y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3 are numpy arrays\n",
    "expected_values = [[-0.8365], [-0.47], [0.54], [0.0005], [0.], [0.54], [0.0005], [0.], [-0.54], [0.0035]]\n",
    "\n",
    "arrays_to_check = [y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3]\n",
    "\n",
    "for array, expected in zip(arrays_to_check, expected_values):\n",
    "    array_expected = np.array(expected)\n",
    "    assert np.allclose(array, array_expected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "CAr7n1lixuhQ",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "37ba893484532446292eb18af5f9c26f",
     "grade": false,
     "grade_id": "cell-3daa40dcd935f324",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the shallow neural network.  We'll assume input in is range [0,1] and output [-1,1]\n",
    "# If the plot_all flag is set to true, then we'll plot all the intermediate stages as in Figure 3.3 \n",
    "def plot_neural(x, y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3, plot_all=False, x_data=None, y_data=None):\n",
    "\n",
    "  # Plot intermediate plots if flag set\n",
    "  if plot_all:\n",
    "    fig, ax = plt.subplots(3,3)\n",
    "    fig.set_size_inches(8.5, 8.5)\n",
    "    fig.tight_layout(pad=3.0)\n",
    "    ax[0,0].plot(x,pre_1,'r-'); ax[0,0].set_ylabel('Preactivation')\n",
    "    ax[0,1].plot(x,pre_2,'b-'); ax[0,1].set_ylabel('Preactivation')\n",
    "    ax[0,2].plot(x,pre_3,'g-'); ax[0,2].set_ylabel('Preactivation')\n",
    "    ax[1,0].plot(x,act_1,'r-'); ax[1,0].set_ylabel('Activation')\n",
    "    ax[1,1].plot(x,act_2,'b-'); ax[1,1].set_ylabel('Activation')\n",
    "    ax[1,2].plot(x,act_3,'g-'); ax[1,2].set_ylabel('Activation')\n",
    "    ax[2,0].plot(x,w_act_1,'r-'); ax[2,0].set_ylabel('Weighted Act')\n",
    "    ax[2,1].plot(x,w_act_2,'b-'); ax[2,1].set_ylabel('Weighted Act')\n",
    "    ax[2,2].plot(x,w_act_3,'g-'); ax[2,2].set_ylabel('Weighted Act')\n",
    "\n",
    "    for plot_y in range(3):\n",
    "      for plot_x in range(3):\n",
    "        ax[plot_y,plot_x].set_xlim([0,1]);ax[plot_x,plot_y].set_ylim([-1,1])\n",
    "        ax[plot_y,plot_x].set_aspect(0.5)\n",
    "      ax[2,plot_y].set_xlabel('Input, $x$');\n",
    "    plt.show()\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.plot(x,y)\n",
    "  ax.set_xlabel('Input, $y$'); ax.set_ylabel('Output, $y$')\n",
    "  ax.set_xlim([0,1]);ax.set_ylim([-1,1])\n",
    "  ax.set_aspect(0.5)\n",
    "  if x_data is not None:\n",
    "    ax.plot(x_data, y_data, 'mo')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "SzIVdp9U-JWb",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5a9b702fb2ddabd6876a381b3786344d",
     "grade": false,
     "grade_id": "cell-ae3c76a9a04d5f7b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Now lets define some parameters and run the neural network\n",
    "theta_10 =  0.3 ; theta_11 = -1.0\n",
    "theta_20 = -1.0  ; theta_21 = 2.0\n",
    "theta_30 = -0.5  ; theta_31 = 0.65\n",
    "phi_0 = -0.3; phi_1 = 2.0; phi_2 = -1.0; phi_3 = 7.0\n",
    "\n",
    "# Define a range of input values\n",
    "x = np.arange(0,1,0.01)\n",
    "\n",
    "# We run the neural network for each of these input values\n",
    "y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3 = \\\n",
    "    shallow_1_1_3(x, ReLU, phi_0,phi_1,phi_2,phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31)\n",
    "# And then plot it\n",
    "plot_neural(x, y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3, plot_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec7f368a0dac78a847f961a6bfa72e10",
     "grade": false,
     "grade_id": "cell-856f167ef84a048c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Note and question:** If look at the code in the previous cell, you see that `x` is a numpy array but we suggested\n",
    "> you treat `x` as a scalar inside the function `shallow_1_1_3()`. What happened?\n",
    ">\n",
    "> **Answer:** This works because numpy uses a concept called broadcasting. If your function only had scalar operations\n",
    "> and nominally one of the operations is a numpy array, then numpy will automatically repeat the scalar operation for \n",
    "> every element of the numpy array. This is a very handy feature and in fact is much more performant that implementing\n",
    "> for-loops for large arrays. See [numpy broadcasting guide](https://numpy.org/doc/stable/user/basics.broadcasting.html)\n",
    "> for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "jhaBSS8oIWSX",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "602e6facd672c9b84424f16063fdb709",
     "grade": false,
     "grade_id": "cell-55df892202279c23",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's play with the parameters to make sure we understand how they work.  The original  parameters were:\n",
    "\n",
    "$\\theta_{10} =  0.3$ ; $\\theta_{20} = -1.0$<br>\n",
    "$\\theta_{20} =  -1.0$ ; $\\theta_{21} = 2.0$<br>\n",
    "$\\theta_{30} =  -0.5$ ; $\\theta_{31} = 0.65$<br>\n",
    "$\\phi_0 = -0.3; \\phi_1 = 2.0; \\phi_2 = -1.0; \\phi_3 = 7.0$\n",
    "\n",
    "The following cell is for you own understanding. It will not be graded. Nonetheless try the suggestions and see if\n",
    "can predict correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur4arJ8KAQWe"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# 1. Predict what effect changing phi_0 will have on the network.  \n",
    "#   Answer:\n",
    "# 2. Predict what effect multiplying phi_1, phi_2, phi_3 by 0.5 would have.  Check if you are correct\n",
    "#   Answer:\n",
    "# 3. Predict what effect multiplying phi_1 by -1 will have.  Check if you are correct.\n",
    "#   Answer:\n",
    "# 4. Predict what effect setting theta_20 to -1.2 will have.  Check if you are correct.\n",
    "#   Answer:\n",
    "# 5. Change the parameters so that there are only two \"joints\" (including outside the range of the plot) \n",
    "# There are actually three ways to do this. See if you can figure them all out\n",
    "# Answer:\n",
    "# 6. With the original parameters, the second line segment is flat (i.e. has slope zero)\n",
    "# How could you change theta_10 so that all of the segments have non-zero slopes\n",
    "# Answer:\n",
    "# 7. What do you predict would happen if you multiply theta_20 and theta21 by 0.5, and phi_2 by 2.0?\n",
    "# Check if you are correct.\n",
    "# Answer:\n",
    "# 8. What do you predict would happen if you multiply theta_20 and theta21 by -0.5, and phi_2 by -2.0?\n",
    "# Check if you are correct.\n",
    "# Answer:\n",
    "\n",
    "theta_10 =  0.3 ; theta_11 = -1.0\n",
    "theta_20 = -1.0  ; theta_21 = 2.0\n",
    "theta_30 = -0.5  ; theta_31 = 0.65\n",
    "phi_0 = -0.3; phi_1 = 2.0; phi_2 = -1.0; phi_3 = 7.0\n",
    "\n",
    "# Define a range of input values\n",
    "x = np.arange(0,1,0.01)\n",
    "\n",
    "# We run the neural network for each of these input values\n",
    "y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3 = \\\n",
    "    shallow_1_1_3(x, ReLU, phi_0,phi_1,phi_2,phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31)\n",
    "# And then plot it\n",
    "plot_neural(x, y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3, plot_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "1NTT5GTbJSqK",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed4ee676ba7239cddf27c8079a3a2e4e",
     "grade": false,
     "grade_id": "cell-fbd914beb8322bcf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Different activation functions\n",
    "\n",
    "The ReLU isn't the only kind of activation function.  For a long time, people used sigmoid functions.  A logistic sigmoid function is defined by the equation\n",
    "\n",
    "\\begin{equation}\n",
    "f[h] = \\frac{1}{1+\\exp{[-10 z ]}}\n",
    "\\end{equation}\n",
    "\n",
    "(Note that the factor of 10 is not standard -- but it allows us to plot on the same axes as the ReLU examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "FEzzQeVoZdV_",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "00e2c7bb9ec8d6b7881dd91e6060a9c5",
     "grade": false,
     "grade_id": "cell-ca1d6ba047ab569c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the sigmoid function\n",
    "def sigmoid(preactivation):\n",
    "  # write code to implement the sigmoid function and compute the activation at the \n",
    "  # hidden unit from the 10*preactivation.  Use the np.exp() function.\n",
    "  # Again, multiply preactivation by to in your calculation for this homework\n",
    "  # just to make the plots align with the ReLU plots.\n",
    "\n",
    "  # YOUR CODE HERE\n",
    "  raise NotImplementedError()\n",
    "\n",
    "  return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0256ffa6feb76ba4b96eb4dad5d8c53",
     "grade": true,
     "grade_id": "cell-f1592faf5aeef0d6",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's test your sigmoid implementation\n",
    "test_z = np.arange(-1, 1, 0.33)\n",
    "test_sig_z = sigmoid(test_z)\n",
    "test_sig_z_expected = np.array([4.53978687e-05, 1.22939862e-03, 3.22954647e-02, 4.75020813e-01, 9.60834277e-01, 9.98498818e-01, 9.99944551e-01])\n",
    "assert np.allclose(test_sig_z, test_sig_z_expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "285d99693ad22a57bdb40fecfbff3dc2",
     "grade": false,
     "grade_id": "cell-86f63d1c3d2e4d04",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's plot your modified sigmoid for $-1 \\le x \\le 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "dIn42wDlKqsv",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5fbd0254c1a0ed2bc3b1c206e5a0b3e9",
     "grade": false,
     "grade_id": "cell-3a32ef192b6d3207",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Make an array of inputs\n",
    "z = np.arange(-1,1,0.01)\n",
    "sig_z = sigmoid(z)\n",
    "\n",
    "# Plot the sigmoid function\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(z,sig_z,'r-')\n",
    "ax.set_xlim([-1,1]);ax.set_ylim([0,1])\n",
    "ax.set_xlabel('z'); ax.set_ylabel('sig[z]')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "uwQHGdC5KpH7",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "431fb894ef3c65b203c4d40917106104",
     "grade": false,
     "grade_id": "cell-3f45a321ffa6be65",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's see what happens when we use this activation function in a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5W9m9MLKLddi",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c20bd44b9e688a24cb0ef4e410980f50",
     "grade": false,
     "grade_id": "cell-46d9d9181868fa4c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "theta_10 =  0.3 ; theta_11 = -1.0\n",
    "theta_20 = -1.0  ; theta_21 = 2.0\n",
    "theta_30 = -0.5  ; theta_31 = 0.65\n",
    "phi_0 = 0.3; phi_1 = 0.5; phi_2 = -1.0; phi_3 = 0.9\n",
    "\n",
    "# Define a range of input values\n",
    "x = np.arange(0,1,0.01)\n",
    "\n",
    "# We run the neural network for each of these input values\n",
    "y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3 = \\\n",
    "    shallow_1_1_3(x, sigmoid, phi_0,phi_1,phi_2,phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31)\n",
    "# And then plot it\n",
    "plot_neural(x, y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3, plot_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "0c4S-XfnSfDx",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "adb78653f1ebf88e78712e49400dd631",
     "grade": false,
     "grade_id": "cell-5e98ae1869fa47b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You probably notice that this gives nice smooth curves.  So why don't we use this?  Aha... it's not obvious right now, but we will get to it when we learn to fit models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "IA_v_-eLRqek",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "445c75b3371279a189c4239ed4c6d12e",
     "grade": false,
     "grade_id": "cell-7a56cbdda5176cbe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Linear activation functions\n",
    "\n",
    "However, neural networks don't work if the activation function is linear.  For example, consider what would happen if the activation function was: \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{lin}[z] = a + bz\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "fTHJRv0KLjMD",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c7f15e3082706afed47f6114eaf6a265",
     "grade": false,
     "grade_id": "cell-54f77d4f5d3b4ea1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the linear activation function\n",
    "def lin(preactivation):\n",
    "  a =0\n",
    "  b =1\n",
    "\n",
    "  # Compute linear function\n",
    "  activation = a + b * preactivation\n",
    "  \n",
    "  # Return\n",
    "  return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "39e4f190eb1731fa754596c0fb022631",
     "grade": false,
     "grade_id": "cell-12d2722202425cea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Self-Study Questions\n",
    "\n",
    "1. The linear activation function above just returns the input: (0+1*z) = z.\n",
    "    1. Before running the code below,  make a prediction about what the ten panels of\n",
    "       the drawing will look like. \n",
    "    2. Now run the code below to see if you were right. What family of functions can this represent?  \n",
    " \n",
    "2. What happens if you change the parameters (a,b) in `lin()` to different values?  \n",
    "   Try a=0.5, b=-0.4.  Don't forget to run the cell again to update the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "SauRG8r7TkvP",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3df308b6eaf63491a6ab1df5d96ad28c",
     "grade": false,
     "grade_id": "cell-18ec8c82f15de749",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "theta_10 =  0.3 ; theta_11 = -1.0\n",
    "theta_20 = -1.0  ; theta_21 = 2.0\n",
    "theta_30 = -0.5  ; theta_31 = 0.65\n",
    "phi_0 = 0.3; phi_1 = 0.5; phi_2 = -1.0; phi_3 = 0.9\n",
    "\n",
    "# Define a range of input values\n",
    "x = np.arange(0,1,0.01)\n",
    "\n",
    "# We run the neural network for each of these input values\n",
    "y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3 = \\\n",
    "    shallow_1_1_3(x, lin, phi_0,phi_1,phi_2,phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31)\n",
    "# And then plot it\n",
    "plot_neural(x, y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3, plot_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "610ccb265cd9a9ef8a1188a8f87d9a56",
     "grade": false,
     "grade_id": "cell-ecd68fed3e57db07",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Hopefully it is clear why using a nonlinear activation function is so important!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "osonHsEqVp2I",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c28fe8b1b98169fae562d3e2edbc3836",
     "grade": false,
     "grade_id": "cell-d41147510bba89be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Least squares loss\n",
    "\n",
    "Now let's consider fitting the network to data.  First we need to define the loss function.  We'll use the least squares loss:\n",
    "\n",
    "\\begin{equation}\n",
    "L[\\boldsymbol\\phi] = \\sum_{i=1}^{I}(y_{i}-\\mathrm{f}[x_{i},\\boldsymbol\\phi])^2\n",
    "\\end{equation}\n",
    "\n",
    "where $(x_i,y_i)$ is an input/output training pair and $\\mathrm{f}[\\bullet,\\boldsymbol\\phi]$ is the neural network with parameters $\\boldsymbol\\phi$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "14d5II-TU46w",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0bbcf644a089be0b30efd7883ba884d0",
     "grade": false,
     "grade_id": "cell-a20ded864173f14c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Least squares function\n",
    "def least_squares_loss(y_train, y_predict):\n",
    "  # Compute the sum of squared\n",
    "  # differences between the real and predicted values of y\n",
    "  # you will need to use the function np.sum\n",
    "\n",
    "  # YOUR CODE HERE\n",
    "  raise NotImplementedError()\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "13d674641250308f5f781f65da15954b",
     "grade": true,
     "grade_id": "cell-44c28cd2adce419c",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test your function with dummy data\n",
    "\n",
    "y_train = np.array([0.47054415, 0.13747013, 0.71269093, 0.01539034, 0.63096982, 0.98647489,\n",
    "                    0.12939223, 0.16237443, 0.37540949, 0.89636868])\n",
    "y_predict = np.array([0.68049052, 0.12401763, 0.5127831,  0.20070157, 0.37388425, 0.151199,\n",
    "                         0.76840562, 0.24458121, 0.60318515, 0.26141021])\n",
    "\n",
    "loss = least_squares_loss(y_train, y_predict)\n",
    "assert np.isclose(loss, 1.752490)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4b206df8378eb031f46b931de544bfac",
     "grade": false,
     "grade_id": "cell-5dc555fe5d48a22a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Manually Train a Shallow Network\n",
    "\n",
    "Now we're going to define a toy dataset and manually train a shallow network with\n",
    "1 input, 3 hidden units and 1 output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ba00573c146002b4743a25a64a39d6c",
     "grade": false,
     "grade_id": "cell-267c85823c39a990",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Manually define our training data\n",
    "x_train = np.array([0.09291784,0.46809093,0.93089486,0.67612654,0.73441752,0.86847339,\\\n",
    "                   0.49873225,0.51083168,0.18343972,0.99380898,0.27840809,0.38028817,\\\n",
    "                   0.12055708,0.56715537,0.92005746,0.77072270,0.85278176,0.05315950,\\\n",
    "                   0.87168699,0.58858043])\n",
    "y_train = np.array([-0.15934537,0.18195445,0.451270150,0.13921448,0.09366691,0.30567674,\\\n",
    "                    0.372291170,0.40716968,-0.08131792,0.41187806,0.36943738,0.3994327,\\\n",
    "                    0.019062570,0.35820410,0.452564960,-0.0183121,0.02957665,-0.24354444, \\\n",
    "                    0.148038840,0.26824970])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a1768e91cc591ffed3762c91ca23b3ea",
     "grade": false,
     "grade_id": "cell-ccc2fa763fd5f14d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the cell below, we do the following:\n",
    "1. Assign initial parameters to the $\\phi$'s and $\\theta$'s, which you will\n",
    "   subsequently change.\n",
    "2. Plot your model with the parameters you assigned.\n",
    "3. Execute your model on the training data inputs to infer output predictions.\n",
    "4. Calculate the squared loss between your model predictions and the training\n",
    "   data outputs.\n",
    "\n",
    "The **homework exercise** is try to get the loss below 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o6GXjtRubZ2U"
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# 1. Define parameters values\n",
    "################################\n",
    "\n",
    "# thetas of the linear models\n",
    "theta_10 =  0.3 ; theta_11 = -1.0   # y-intercept and slope of first linear unit\n",
    "theta_20 = -1.0  ; theta_21 = 2.0   # y-intercept and slope of second linear unit\n",
    "theta_30 = -0.5  ; theta_31 = 0.65  # y-intercept and slope of third linear unit\n",
    "\n",
    "# y-intercept and weights for each of the 3 hidden units\n",
    "phi_0 = -0.3; phi_1 = 2.0; phi_2 = -1.0; phi_3 = 7.0\n",
    "\n",
    "################################\n",
    "# 2. Plot your model\n",
    "################################\n",
    "\n",
    "# Define a range of input values\n",
    "x = np.arange(0,1,0.01)\n",
    "\n",
    "# We run the neural network for each of these input values\n",
    "y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3 = \\\n",
    "    shallow_1_1_3(x, ReLU, phi_0,phi_1,phi_2,phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31)\n",
    "# And then plot it\n",
    "plot_neural(x, y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3, plot_all=True, x_data = x_train, y_data = y_train)\n",
    "\n",
    "#########################################################################\n",
    "# 3. Run the neural network on the training data and calculate the loss\n",
    "#########################################################################\n",
    "\n",
    "y_predict, *_ = shallow_1_1_3(x_train, ReLU, phi_0,phi_1,phi_2,phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31)\n",
    "\n",
    "# Compute the least squares loss and print it out\n",
    "loss = least_squares_loss(y_train,y_predict)\n",
    "print(\"Loss = %3.3f\"%(loss))\n",
    "\n",
    "# TODO.  Manipulate the parameters (by hand!) to make the function \n",
    "# fit the data better and try to reduce the loss to be less than 0.2\n",
    "# Tip... start by manipulating phi_0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f8ff499db5c2435e2e35f54bf00b3a8",
     "grade": true,
     "grade_id": "cell-60b8944df318ab35",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test your model loss\n",
    "\n",
    "assert loss < 0.2,  \"Try to reduce the loss to be less than 0.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "SSQuNtAcisHs",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ecb886855922277372fe3a76296be77",
     "grade": false,
     "grade_id": "cell-80f628e81560eec8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Networks with two inputs\n",
    "\n",
    "Now we'll build a neural network that takes two inputs similar to figure 3.8.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "tyfGcDq_bcQy",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0f096bea65911e8330cdfdd26c85c4ca",
     "grade": false,
     "grade_id": "cell-41844c6db04a3a06",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Code to draw 2D function -- read it so you know what is going on, but you don't have to change it\n",
    "def draw_2D_function(ax, x1_mesh, x2_mesh, y, draw_heatmap=False):\n",
    "    pos = ax.contourf(x1_mesh, x2_mesh, y, levels=256 ,cmap = 'hot', vmin=-10,vmax=10.0)\n",
    "    if draw_heatmap:\n",
    "      fig.colorbar(pos, ax=ax)\n",
    "    ax.set_xlabel('x1');ax.set_ylabel('x2')\n",
    "    levels = np.arange(-10,10,1.0)\n",
    "    ax.contour(x1_mesh, x2_mesh, y, levels, cmap='winter')\n",
    "\n",
    "# Plot the shallow neural network.  We'll assume input in is range [0,10],[0,10] and output [-10,10]\n",
    "# If the plot_all flag is set to true, then we'll plot all the intermediate stages as in Figure 3.3 \n",
    "def plot_neural_2_inputs(x1,x2, y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3, plot_all=False):\n",
    "\n",
    "  # Plot intermediate plots if flag set\n",
    "  if plot_all:\n",
    "    fig, ax = plt.subplots(3,3)\n",
    "    fig.set_size_inches(8.5, 8.5)\n",
    "    fig.tight_layout(pad=3.0)\n",
    "    draw_2D_function(ax[0,0], x1,x2,pre_1); ax[0,0].set_title('Preactivation')\n",
    "    draw_2D_function(ax[0,1], x1,x2,pre_2); ax[0,1].set_title('Preactivation')\n",
    "    draw_2D_function(ax[0,2], x1,x2,pre_3); ax[0,2].set_title('Preactivation')\n",
    "    draw_2D_function(ax[1,0], x1,x2,act_1); ax[1,0].set_title('Activation')\n",
    "    draw_2D_function(ax[1,1], x1,x2,act_2); ax[1,1].set_title('Activation')\n",
    "    draw_2D_function(ax[1,2], x1,x2,act_3); ax[1,2].set_title('Activation')\n",
    "    draw_2D_function(ax[2,0], x1,x2,w_act_1); ax[2,0].set_title('Weighted Act')\n",
    "    draw_2D_function(ax[2,1], x1,x2,w_act_2); ax[2,1].set_title('Weighted Act')\n",
    "    draw_2D_function(ax[2,2], x1,x2,w_act_3); ax[2,2].set_title('Weighted Act')\n",
    "    plt.show()\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  draw_2D_function(ax,x1,x2,y,draw_heatmap=True)\n",
    "  ax.set_title('Network ouptut, $y$')\n",
    "  ax.set_aspect(1.0)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "F8T9kYuBpBKO",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "48cd94100286ded0544a7c9b56fb1958",
     "grade": false,
     "grade_id": "cell-0fe5acf8be60f0a2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define a shallow neural network with, two inputs, one output, and three hidden units\n",
    "def shallow_2_1_3(x1,x2, activation_fn, \n",
    "                  phi_0,phi_1,phi_2,phi_3, \n",
    "                  theta_10, theta_11, theta_12, \n",
    "                  theta_20, theta_21, theta_22, \n",
    "                  theta_30, theta_31, theta_32):\n",
    "  \n",
    "  # Compute pre_1, pre_2 and pre_3 from three linear\n",
    "  # functions (figure 3.8a-c) with the theta parameters.  \n",
    "  # These are the preactivations\n",
    "\n",
    "  # YOUR CODE HERE\n",
    "  raise NotImplementedError()\n",
    "\n",
    "  # Pass these through the activation_fn to compute the activations as in \n",
    "  # figure 3.8 d-f.\n",
    "\n",
    "  act_1 = activation_fn(pre_1)\n",
    "  act_2 = activation_fn(pre_2)\n",
    "  act_3 = activation_fn(pre_3)\n",
    "\n",
    "  # Writ the code below to weight the activations (a.k.a. hidden units) using\n",
    "  # phi1, phi2 and phi3 and assign them to w_act_1, w_act_2 and w_act_3\n",
    "  # to create the equivalent of figure 3.8 g-i\n",
    "\n",
    "  # YOUR CODE HERE\n",
    "  raise NotImplementedError()\n",
    "  \n",
    "  # Write the code below to combine the weighted activations and add \n",
    "  # phi_0 and assign it to y to create the output as in figure 3.8j\n",
    "\n",
    "  # YOUR CODE HERE\n",
    "  raise NotImplementedError()\n",
    "  \n",
    "  # Return everything we have calculated\n",
    "  return y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "C8rzVcZnnxia",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef6e625bf0312eced1d56f329ee545c2",
     "grade": false,
     "grade_id": "cell-b2ae36fb855ca7c0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Now lets define some parameters\n",
    "theta_10 =  4.0 ;  theta_11 = -0.9; theta_12 = 0.0\n",
    "theta_20 =  5.0  ; theta_21 = -0.9 ; theta_22 = -0.5\n",
    "theta_30 =  -5  ; theta_31 = 0.5; theta_32 = 0.9\n",
    "\n",
    "phi_0 = 0.0; phi_1 = -1.0; phi_2 = 2.0; phi_3 = 0.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a7b0bf7fddb6bf3d08ccd013f9ba41dd",
     "grade": true,
     "grade_id": "cell-51afa3ea1480ad93",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's test that your model is producing correct results\n",
    "\n",
    "x1 = np.array([0.77])\n",
    "x2 = np.array([0.33])\n",
    "\n",
    "y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3 = \\\n",
    "    shallow_2_1_3(x1,x2, ReLU, phi_0,phi_1,phi_2,phi_3, theta_10, theta_11, theta_12, theta_20, theta_21, theta_22, theta_30, theta_31, theta_32)\n",
    "\n",
    "expected_values = [[4.977], [3.307], [4.142], [-4.318], [3.307], [4.142], [0.], [-3.307], [8.284], [0.]]\n",
    "arrays_to_check = [y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3]\n",
    "\n",
    "for array, expected in zip(arrays_to_check, expected_values):\n",
    "    array_expected = np.array(expected)\n",
    "    assert np.allclose(array, array_expected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "816afd7079af2bb7578f1ffdd4ef8673",
     "grade": false,
     "grade_id": "cell-9deefe3b8c7f2e18",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "x1 = np.arange(0.0, 10.0, 0.1)\n",
    "x2 = np.arange(0.0, 10.0, 0.1)\n",
    "x1,x2 = np.meshgrid(x1,x2)  # https://www.geeksforgeeks.org/numpy-meshgrid-function/\n",
    "\n",
    "# We run the neural network for each of these input values\n",
    "y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3 = \\\n",
    "    shallow_2_1_3(x1,x2, ReLU, phi_0,phi_1,phi_2,phi_3, theta_10, theta_11, theta_12, theta_20, theta_21, theta_22, theta_30, theta_31, theta_32)\n",
    "# And then plot it\n",
    "plot_neural_2_inputs(x1,x2, y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3, plot_all=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOxu3uluisAmtb8nx5pmeKr",
   "collapsed_sections": [],
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
