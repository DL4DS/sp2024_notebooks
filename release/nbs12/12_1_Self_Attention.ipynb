{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While editing this notebook, don't change cell types as that confuses the autograder.\n",
    "\n",
    "Before you turn this notebook in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel $\\rightarrow$ Restart) and then **run all cells** (in the menubar, select Cell $\\rightarrow$ Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Understanding Deep Learning_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "view-in-github",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7feeeab44b6430eaad972561457c0110",
     "grade": false,
     "grade_id": "cell-73fa70b6ee17c75a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/udlbook/udlbook/blob/main/Notebooks/Chap12/12_1_Self_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "t9vk9Elugvmi",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d58a5cdb067ad4fc5c69880c5b150f16",
     "grade": false,
     "grade_id": "cell-f3abc25bc84dd6ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Notebook 12.1: Self Attention\n",
    "\n",
    "This notebook builds a self-attention mechanism from scratch, as discussed in section 12.2 of the book.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "OLComQyvCIJ7",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7646d5c1b6a8e4d8452e7aeef6ef34c1",
     "grade": false,
     "grade_id": "cell-74183fac4792f262",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cc671edd70a49df03ad0eee56e314524",
     "grade": false,
     "grade_id": "cell-806bfda2c810a0ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "9OJkkoNqCVK2",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f7d26f5e16da20e256b5a0b412f81d1e",
     "grade": false,
     "grade_id": "cell-20f76c8769656fc8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Define the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "oAygJwLiCSri",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "579851a2dd53b4920571cd08e8c2e8e7",
     "grade": false,
     "grade_id": "cell-7bc4e41acc8f23dd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Set seed so we get the same random numbers\n",
    "np.random.seed(3)\n",
    "\n",
    "# Number of inputs\n",
    "N = 3\n",
    "# Number of dimensions of each input\n",
    "D = 4\n",
    "\n",
    "# Create an empty list\n",
    "all_x = []\n",
    "\n",
    "# Create elements x_n and append to list\n",
    "for n in range(N):\n",
    "  all_x.append(np.random.normal(size=(D,1)))\n",
    "\n",
    "# all_x is now a list of N Dx1 numpy arrays\n",
    "  \n",
    "# Print out the list\n",
    "for n in range(len(all_x)):\n",
    "  print(\"Input number\", n, \"is:\")\n",
    "  print(all_x[n])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "W2iHFbtKMaDp",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "de9a372b6bdd42e682b81afae8f2566f",
     "grade": false,
     "grade_id": "cell-1403026d4cc4eb84",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We'll also need the weights and biases for the keys, queries, and values (equations 12.2 and 12.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "79TSK7oLMobe",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f474b6908403fe2c0cd525c40ed87f9c",
     "grade": false,
     "grade_id": "cell-f5245c6e4cbad6a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Set seed so we get the same random numbers\n",
    "np.random.seed(0)\n",
    "\n",
    "# Choose random values for the parameters\n",
    "omega_q = np.random.normal(size=(D,D))\n",
    "omega_k = np.random.normal(size=(D,D))\n",
    "omega_v = np.random.normal(size=(D,D))\n",
    "beta_q = np.random.normal(size=(D,1))\n",
    "beta_k = np.random.normal(size=(D,1))\n",
    "beta_v = np.random.normal(size=(D,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b65b2bfcc1c51d11a99138f01191ed54",
     "grade": false,
     "grade_id": "cell-4b1e8e8c52881610",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Calculating Attention via Matrix Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "PJ2vCQ_7C38K",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "92bdccbfa1e334e21321cde5ee48de13",
     "grade": false,
     "grade_id": "cell-adf6b807683d1a5b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's compute attention using matrix calculations.  We'll store the $N$ inputs $\\mathbf{x}_{n}\\in\\mathbb{R}^{D}$ in the columns of a $D\\times N$ matrix, using equations 12.6 and 12.7/8.\n",
    "\n",
    "Note:  The book uses column vectors (for compatibility with the rest of the text), but in the wider literature it is more normal to store the inputs in the rows of a matrix;  in this case, the computation is the same, but all the matrices are transposed and the operations proceed in the reverse order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "obaQBdUAMXXv",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "721c85cab10bed06ff471a60edaf42fa",
     "grade": false,
     "grade_id": "cell-74419001f5aa7902",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define softmax operation that works independently on each column\n",
    "def softmax_cols(data_in):\n",
    "  # Exponentiate all of the values\n",
    "  exp_values = np.exp(data_in) ;\n",
    "  # Sum over columns\n",
    "  denom = np.sum(exp_values, axis = 0);\n",
    "  # Replicate denominator to N rows\n",
    "  denom = np.matmul(np.ones((data_in.shape[0],1)), denom[np.newaxis,:])\n",
    "  # Compute softmax\n",
    "  softmax = exp_values / denom\n",
    "  # return the answer\n",
    "  return softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "gb2WvQ3SiH8r",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "72d7a791c16be28eecd978d77e81c9d3",
     "grade": false,
     "grade_id": "cell-73647b1bde49a7c6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    " # Now let's compute self attention in matrix form\n",
    "def self_attention(X,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k):\n",
    "  \"\"\"\n",
    "  Calculate self-attention via matrix calculations.\n",
    "  \n",
    "  Returns:\n",
    "    X_prime: output matrix\n",
    "    attentions: the attention matrix\n",
    "  \"\"\"\n",
    "\n",
    "  # TODO -- Write this function\n",
    "  # 1. Compute queries, keys, and values\n",
    "  # 2. Compute dot products\n",
    "  # 3. Apply softmax to calculate attentions\n",
    "  # 4. Weight values by attentions\n",
    "  # 5. Return X_prime and attentions\n",
    "\n",
    "  # YOUR CODE HERE\n",
    "  raise NotImplementedError()\n",
    "\n",
    "\n",
    "  return X_prime, attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "MUOJbgJskUpl",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8db7e9b662244f417bad9bcbd0752624",
     "grade": false,
     "grade_id": "cell-b2e8b9d6ba3a0e59",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Copy data into matrix\n",
    "X = np.zeros((D, N))\n",
    "X[:,0] = np.squeeze(all_x[0])\n",
    "X[:,1] = np.squeeze(all_x[1])\n",
    "X[:,2] = np.squeeze(all_x[2])\n",
    "\n",
    "# Run the self attention mechanism\n",
    "X_prime, attentions = self_attention(X,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k)\n",
    "\n",
    "# print attentions to 3 decimal places\n",
    "print(\"attentions = \", np.round(attentions, 3))\n",
    "\n",
    "# Print out the results\n",
    "print(\"\\nX_prime = \", X_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a475ae13f7333e590b4ab8e9fa144af8",
     "grade": true,
     "grade_id": "cell-a388a00cef9c39c8",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell -- Do not edit\n",
    "\n",
    "x_prime_true = np.array([[ 0.94744244, -0.24348429, -0.91310441, -0.44522983],\n",
    "    [ 1.64201168, -0.08470004,  4.02764044,  2.18690791],\n",
    "    [ 1.61949281, -0.06641533,  3.96863308,  2.15858316]]).transpose()\n",
    "\n",
    "assert np.allclose(X_prime, x_prime_true)\n",
    "\n",
    "attentions_true = np.array([[0., 0., 0.005], \n",
    "                            [0.998, 0.006, 0.007], \n",
    "                            [0.002, 0.994, 0.988]])\n",
    "\n",
    "assert np.allclose(attentions, attentions_true, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "as_lRKQFpvz0",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b4bf31ec156cfff7d803e622de46092",
     "grade": false,
     "grade_id": "cell-4ad6c7a8c7d0806d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Note the print out of the attention matrix. \n",
    "You will see that the values are quite extreme. One element of each column is very\n",
    "close to one and the others are very close to zero.  \n",
    "\n",
    "Now we'll fix this problem by using scaled dot-product attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "338cde3dea4d853ab8377f7d72b285b9",
     "grade": false,
     "grade_id": "cell-8c9c0b68aea35e34",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Scaled Dot-Product Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "kLU7PUnnqvIh",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5bc346c528d80a35ba78e16168fef825",
     "grade": false,
     "grade_id": "cell-adf3a8d3228e607d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Now let's compute self attention in matrix form\n",
    "def scaled_dot_product_self_attention(X,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k):\n",
    "  \"\"\"\n",
    "  Calculate scaled self-attention via matrix calculations.\n",
    "  \n",
    "  Returns:\n",
    "    X_prime: output matrix\n",
    "    attentions: the attention matrix\n",
    "  \"\"\"\n",
    "\n",
    "  # TODO -- Write this function\n",
    "  # 1. Compute queries, keys, and values\n",
    "  # 2. Compute dot products\n",
    "  # 3. Scale the dot products as in equation 12.9\n",
    "  # 4. Apply softmax to calculate attentions\n",
    "  # 5. Weight values by attentions\n",
    "  # 6. Return X_prime and attentions\n",
    "\n",
    "  # YOUR CODE HERE\n",
    "  raise NotImplementedError()\n",
    "\n",
    "  return X_prime, attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "n18e3XNzmVgL",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a079af7118a9e160be665376f2f48dd",
     "grade": false,
     "grade_id": "cell-597dde9704470f37",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run the self attention mechanism\n",
    "X_prime2, attentions2 = scaled_dot_product_self_attention(X,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k)\n",
    "\n",
    "# print attentions to 3 decimal places\n",
    "print(\"attentions = \", np.round(attentions2, 3))\n",
    "\n",
    "# Print out the results\n",
    "print(\"\\nX_prime = \", X_prime2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b6304b531c939ff713d7ae507b7c8f98",
     "grade": true,
     "grade_id": "cell-98fb86d54272b26f",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell. Do not edit.\n",
    "\n",
    "attentions_true2 = np.array([[0.,    0.,    0.062],\n",
    " [0.96,  0.071, 0.071],\n",
    " [0.04,  0.929, 0.867]])\n",
    "\n",
    "assert np.allclose(attentions2, attentions_true2, atol=1e-3)\n",
    "\n",
    "X_prime_true2 = np.array([[ 0.97411966,  1.59622051,  1.32638014],\n",
    " [-0.23738409, -0.09516106,  0.13062402],\n",
    " [-0.72333202,  3.70194096,  3.02371664],\n",
    " [-0.34413007,  2.01339538,  1.6902419 ]])\n",
    "\n",
    "assert np.allclose(X_prime2, X_prime_true2, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "01ffa56466832bb015e51e964549d908",
     "grade": false,
     "grade_id": "cell-b34b2fb3b97e9396",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Note that the attention matrix is evened a little bit. The dimension of the input is still small, so it won't have a\n",
    "huge effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "QDEkIrcgrql-",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d8347ad70f2b4db308a8b9951b51025c",
     "grade": false,
     "grade_id": "cell-cc0eaac5e9f7fa20",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Equivariant to Column Permutation?\n",
    "\n",
    "Let's investigate whether the self-attention mechanism is equivariant/covariant with respect to permutation.\n",
    "If it is, when we permute the columns of the input matrix $\\mathbf{X}$, the columns of the output matrix $\\mathbf{X}'$ will also be permuted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permute the columns of X\n",
    "X_permuted = X[:,[1,2,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the self attention mechanism\n",
    "X_primeP, attentionsP = scaled_dot_product_self_attention(X_permuted,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k)\n",
    "\n",
    "# print attentions to 3 decimal places\n",
    "print(\"attentions with permuted input = \", np.round(attentionsP, 3))\n",
    "\n",
    "# Print out the results\n",
    "print(\"\\nX_prime with permuted input = \", X_primeP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare by applying the same column permutation to the previous output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_prime2 permuted output: \", X_prime2[:,[1,2,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if they are the same\n",
    "np.allclose(X_primeP, X_prime2[:,[1,2,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2c6229b12fd62288065d91d1d585faa4",
     "grade": false,
     "grade_id": "cell-55849ce9cf478a94",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You see that these should be equivariant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7679542b7d49dcae50b7bf7ba2ed19e5",
     "grade": false,
     "grade_id": "cell-fac3419076f7ae09",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question:**\n",
    "\n",
    "Would this still be equivariant if we added positional encoding?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8b024a1fa4d9674179fd878623c00611",
     "grade": false,
     "grade_id": "cell-06fb4613dea50025",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Type your answer in the cell below**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c2e4f07446ee56414be93b03ce7f79fd",
     "grade": true,
     "grade_id": "cell-d47f6e3bc0dd359e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOKrX9gmuhl9+KwscpZKr3u",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
