{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While editing this notebook, don't change cell types as that confuses the autograder.\n",
    "\n",
    "Before you turn this notebook in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel $\\rightarrow$ Restart) and then **run all cells** (in the menubar, select Cell $\\rightarrow$ Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Understanding Deep Learning_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DL4DS/sp2024_notebooks/blob/main/release/nbs12/12_2_Multihead_Self_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "t9vk9Elugvmi",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0bde745a408ef1d3a7413e5b08fcd7da",
     "grade": false,
     "grade_id": "cell-94e84a822ad04a1d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Notebook 12.2: Multihead Self-Attention\n",
    "\n",
    "This notebook builds a multihead self-attention mechanism as in figure 12.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "OLComQyvCIJ7",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "42f5438334637d6ddf1e426cd027d53a",
     "grade": false,
     "grade_id": "cell-7be89cf218825087",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "9OJkkoNqCVK2",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7ff0a6a657902c10c00f09c489e29b07",
     "grade": false,
     "grade_id": "cell-761b59c6d76acece",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The multihead self-attention mechanism maps $N$ inputs $\\mathbf{x}_{n}\\in\\mathbb{R}^{D}$ and returns $N$ outputs $\\mathbf{x}'_{n}\\in \\mathbb{R}^{D}$.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "oAygJwLiCSri",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "56c9678f9e1df6a57ffb7f4dce2d0be4",
     "grade": false,
     "grade_id": "cell-f052263ca686e1b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Set seed so we get the same random numbers\n",
    "np.random.seed(3)\n",
    "# Number of inputs\n",
    "N = 6\n",
    "# Number of dimensions of each input\n",
    "D = 8\n",
    "\n",
    "# Create an input matrix\n",
    "X = np.random.normal(size=(D,N))\n",
    "\n",
    "# Print X\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "W2iHFbtKMaDp",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fdf4657b59ca1f8c1af83e1677c028d7",
     "grade": false,
     "grade_id": "cell-3f98094530ef5988",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We'll use two heads.  We'll need the weights and biases for the keys, queries, and values (equations 12.2 and 12.4).  We'll use two heads, and (as in the figure), we'll make the queries keys and values of size D/H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "79TSK7oLMobe",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff35f5f0e6542cb8d57cae068be48ea9",
     "grade": false,
     "grade_id": "cell-3e821d01aff1bfb5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Number of heads\n",
    "H = 2\n",
    "# QDV dimension\n",
    "H_D = int(D/H)\n",
    "\n",
    "# Set seed so we get the same random numbers\n",
    "np.random.seed(0)\n",
    "\n",
    "# Choose random values for the parameters for the first head\n",
    "omega_q1 = np.random.normal(size=(H_D,D))\n",
    "omega_k1 = np.random.normal(size=(H_D,D))\n",
    "omega_v1 = np.random.normal(size=(H_D,D))\n",
    "beta_q1 = np.random.normal(size=(H_D,1))\n",
    "beta_k1 = np.random.normal(size=(H_D,1))\n",
    "beta_v1 = np.random.normal(size=(H_D,1))\n",
    "\n",
    "# Choose random values for the parameters for the second head\n",
    "omega_q2 = np.random.normal(size=(H_D,D))\n",
    "omega_k2 = np.random.normal(size=(H_D,D))\n",
    "omega_v2 = np.random.normal(size=(H_D,D))\n",
    "beta_q2 = np.random.normal(size=(H_D,1))\n",
    "beta_k2 = np.random.normal(size=(H_D,1))\n",
    "beta_v2 = np.random.normal(size=(H_D,1))\n",
    "\n",
    "# Choose random values for the parameters\n",
    "omega_c = np.random.normal(size=(D,D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "VxaKQtP3Ng6R",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "21114d15855027e5172ffa63b65544d9",
     "grade": false,
     "grade_id": "cell-6c0e4510e3181d75",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's compute the multiscale self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "obaQBdUAMXXv",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa488f3ccd8e40b162484f0f582c1c00",
     "grade": false,
     "grade_id": "cell-5b67a2420ee752ec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define softmax operation that works independently on each column\n",
    "def softmax_cols(data_in):\n",
    "  # Exponentiate all of the values\n",
    "  exp_values = np.exp(data_in) ;\n",
    "  # Sum over columns\n",
    "  denom = np.sum(exp_values, axis = 0);\n",
    "  # Replicate denominator to N rows\n",
    "  denom = np.matmul(np.ones((data_in.shape[0],1)), denom[np.newaxis,:])\n",
    "  # Compute softmax\n",
    "  softmax = exp_values / denom\n",
    "  # return the answer\n",
    "  return softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "gb2WvQ3SiH8r",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe2f60a194c9b27583c1f4671b9963e9",
     "grade": false,
     "grade_id": "cell-4e4656d21dc6f3cf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    " # Now let's compute self attention in matrix form\n",
    "def multihead_scaled_self_attention(X,omega_v1, omega_q1, omega_k1, beta_v1, beta_q1, beta_k1, omega_v2, omega_q2, omega_k2, beta_v2, beta_q2, beta_k2, omega_c):\n",
    "\n",
    "  # TODO Write the multihead scaled self-attention mechanism.\n",
    "  # 1. Compute the values, queries, and keys for the first head\n",
    "  # 2. Compute the dot products and scale\n",
    "  # 3. Compute the attentions for the first head\n",
    "  # 4. Compute the output for the first head\n",
    "  # 5. Compute the values, queries, and keys for the second head\n",
    "  # 6. Compute the dot products and scale for the second head\n",
    "  # 7. Compute the attentions for the second head\n",
    "  # 8. Compute the output for the second head\n",
    "  # 9. Concatenate the outputs from the two heads and weight them by omega_c to\n",
    "  #    produce the final output\n",
    "\n",
    "  # YOUR CODE HERE\n",
    "  raise NotImplementedError()\n",
    "\n",
    "  return X_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "MUOJbgJskUpl",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "272ba12bbd2fb4398ad7655a9dc7855e",
     "grade": false,
     "grade_id": "cell-d2bd1f55d22574d5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run the self attention mechanism\n",
    "X_prime = multihead_scaled_self_attention(X,omega_v1, omega_q1, omega_k1, beta_v1, beta_q1, beta_k1, omega_v2, omega_q2, omega_k2, beta_v2, beta_q2, beta_k2, omega_c)\n",
    "\n",
    "# Print out the results\n",
    "np.set_printoptions(precision=3)\n",
    "print(\"Your answer:\")\n",
    "print(X_prime)\n",
    "\n",
    "print(\"True values:\")\n",
    "print(\"[[-21.207  -5.373 -20.933  -9.179 -11.319 -17.812]\")\n",
    "print(\" [ -1.995   7.906 -10.516   3.452   9.863  -7.24 ]\")\n",
    "print(\" [  5.479   1.115   9.244   0.453   5.656   7.089]\")\n",
    "print(\" [ -7.413  -7.416   0.363  -5.573  -6.736  -0.848]\")\n",
    "print(\" [-11.261  -9.937  -4.848  -8.915 -13.378  -5.761]\")\n",
    "print(\" [  3.548  10.036  -2.244   1.604  12.113  -2.557]\")\n",
    "print(\" [  4.888  -5.814   2.407   3.228  -4.232   3.71 ]\")\n",
    "print(\" [  1.248  18.894  -6.409   3.224  19.717  -5.629]]\")\n",
    "\n",
    "# If your answers don't match, then make sure that you are doing the scaling, and make sure the scaling value is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0893cf60b3043b34bd91cbeb2e65bd60",
     "grade": true,
     "grade_id": "cell-ba1ee5dc740e78c5",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell. Do not edit\n",
    "\n",
    "X_prime_true = np.array([[-21.207,  -5.373, -20.933,  -9.179, -11.319, -17.812],\n",
    " [ -1.995,   7.906, -10.516,   3.452,   9.863,  -7.24 ],\n",
    " [  5.479,   1.115,   9.244,   0.453,   5.656,   7.089],\n",
    " [ -7.413,  -7.416,   0.363,  -5.573,  -6.736,  -0.848],\n",
    " [-11.261,  -9.937,  -4.848,  -8.915, -13.378,  -5.761],\n",
    " [  3.548,  10.036,  -2.244,   1.604,  12.113,  -2.557],\n",
    " [  4.888,  -5.814,   2.407,   3.228,  -4.232,   3.71 ],\n",
    " [  1.248,  18.894,  -6.409,   3.224,  19.717,  -5.629]])\n",
    "\n",
    "assert np.allclose(X_prime, X_prime_true, rtol=1e-3, atol=1e-3), \"Test failed. Your answer did not match the correct answer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMSk8qTqDYqFnRJVZKlsue0",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
